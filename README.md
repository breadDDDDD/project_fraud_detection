# Fraud Assistant â€” RAG + SQL Analytics Chatbot

An AI-powered analytics and knowledge assistant that supports **conversational Q&A**, **Retrieval-Augmented Generation (RAG)**, and **SQL Intelligence**.  
Built with **Streamlit**, powered by **Gemini**, **Pinecone**, **Ollama embeddings**, and **SQL Server**, and fully containerized via **Docker Compose**.

---

## ğŸš€ What This App Does

This app allows you to:

- Chat conversationally with Gemini
- Ask questions over your documents using RAG
- Ask analytical questions backed by SQL Server
    - LLM generates SQL
    - Query executes
    - LLM explains results

All through a clean Streamlit interface.

---

## âœ¨ Features

- ğŸ–¥ï¸ **Streamlit UI**
- ğŸ¤– **Gemini LLM Responses**
- ğŸ“š **RAG Pipeline with Pinecone**
- ğŸ—„ï¸ **SQL Execution + Result Summaries**
- ğŸ§  **Ollama Embedding Engine**
- ğŸ³ Fully Dockerized

---

## ğŸ—ï¸ Architecture Overview

```
User
 â”‚
 â–¼
 Streamlit UI
 â”‚
 â”œâ”€â”€ Normal Chat (Gemini)
 â”œâ”€â”€ RAG Mode (Gemini + Pinecone + Ollama embeddings)
 â””â”€â”€ SQL Mode (Gemini â†’ SQL â†’ DB Query â†’ Summary)
```

**Docker Compose Includes**
- App Container
- Ollama Embedding Server

**External Services**
- SQL Server
- Pinecone
- Gemini API

---

## âš™ï¸ Environment Setup

Create a **.env** file in the project root.

> ğŸ”¥ This is required or the app will NOT run.

### Required Variables

### ğŸ”¹ SQL Server
```
DB_URL=mssql+pyodbc://USERNAME:PASSWORD@host.docker.internal/DB_NAME?driver=ODBC+Driver+17+for+SQL+Server&Encrypt=no
```

**Notes**
- Must use **SQL Authentication**
- Uses ODBC Driver 17
- `host.docker.internal` lets Docker access your host DB

---

### ğŸ”¹ Pinecone
```
PINECONE_API_KEY=your_pinecone_key
PINECONE_INDEX=fraud-docs
```

**Important**
- Index dimension must match embedding model  
  (for qwen embeddings â†’ **1024**)

---

### ğŸ”¹ Gemini
```
GEMINI_API_KEY=your_gemini_key
GEMINI_MODEL=your_preferred_gemini_model
```

---

### ğŸ”¹ Ollama Embeddings
```
OLLAMA_HOST=http://ollama:11434
OLLAMA_EMBED_MODEL=qwen3-embedding:0.6b
```

---

## ğŸ³ Run with Docker

### Prerequisites
- Docker Desktop installed
- SQL Server running
- .env properly configured

---

### 1ï¸âƒ£ Build & Start
```
docker compose up --build
```

### 2ï¸âƒ£ Start (next time)
```
docker compose up
```

### 3ï¸âƒ£ Stop
```
docker compose down
```

---

## ğŸŒ Access the App
Visit:
```
http://localhost:8501
```

---

## ğŸ¤– Verify Ollama Embeddings

Check installed models:
```
docker exec -it ollama curl http://localhost:11434/api/tags
```

You should see:
```
qwen3-embedding:0.6b
```

If missing, install manually:
```
docker exec -it ollama ollama pull qwen3-embedding:0.6b
```

---

## ğŸ› ï¸ Local Development (No Docker)

If you prefer running locally:

### Install uv
```
pip install uv
```

### Create Virtual Environment
```
uv venv
```

Windows:
```
.venv/Scripts/activate
```

Mac/Linux:
```
source .venv/bin/activate
```

### Install Dependencies
```
uv pip install -r requirements.txt
```

### Run Streamlit
```
streamlit run streamlit_app.py
```

Update `.env`:
```
OLLAMA_HOST=http://localhost:11434
```

Open:
```
http://localhost:8501
```

---

## ğŸ›¡ï¸ Security Notes

- `.env` is ignored via `.dockerignore`
- Secrets are **NOT baked** into the image
- Safe to push repo

---

## ğŸ‰ What You Can Do

Once running, you can:
- ğŸ’¬ Chat normally
- ğŸ“š Switch to RAG mode for document Q&A
- ğŸ§  Use SQL mode for analytics insights
- ğŸ“„ Upload PDFs / Docs to build knowledgebase

Enjoy!
